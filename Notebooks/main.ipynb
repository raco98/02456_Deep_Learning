{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d0c34bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Libraries\n",
    "from pathlib import Path\n",
    "from matplotlib import pyplot as plt\n",
    "from boutdata import collect\n",
    "import math \n",
    "import torch\n",
    "from torch import nn, Tensor\n",
    "from torch.nn.functional import softplus\n",
    "from torch.distributions import Distribution, Bernoulli\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torchvision.transforms import ToTensor\n",
    "import torch.nn.functional as F\n",
    "from functools import reduce\n",
    "from typing import *\n",
    "import matplotlib\n",
    "from IPython.display import Image, display, clear_output\n",
    "import numpy as np\n",
    "%matplotlib nbagg\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "from eval_plots import plot_training_curves, visualize_reconstructions_live\n",
    "from collections import defaultdict\n",
    "from plotting import make_plasma_vae_plots\n",
    "\n",
    "sns.set_style(\"whitegrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8bda3bf9",
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_LOCATION = Path(\n",
    "\tr\"data_numpy/density_large_rolled128.npy\"\n",
    ")\n",
    "\n",
    "data = np.load(DATA_LOCATION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "aace0f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Normalize data\n",
    "max_val = np.max(data)\n",
    "density_data = data/max_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "777eeb76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2004, 1, 128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Get the data in the correct format\n",
    "\n",
    "data_tensor = torch.tensor(density_data, dtype=torch.float32)\n",
    "data_tensor = data_tensor.unsqueeze(1)  # add channel dim: [N, 1, H, W]\n",
    "\n",
    "print(data_tensor.shape) # Ensure data shape matches expectation!\n",
    "\n",
    "# train and validation split\n",
    "total_count = len(data_tensor)\n",
    "train_size = int(0.8 * total_count)\n",
    "test_size = total_count - train_size\n",
    "\n",
    "# Split data_tensor (not data!) so batches have channel dimension\n",
    "train_dataset, test_dataset = random_split(data_tensor, [train_size, test_size])\n",
    "\n",
    "batch_size = 8\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "84d70910",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/p0/74q9vxn91x9b906sr13d_ynw0000gn/T/ipykernel_1373/3864393536.py:5: UserWarning: The use of `x.T` on tensors of dimension other than 2 to reverse their shape is deprecated and it will throw an error in a future release. Consider `x.mT` to transpose batches of matrices or `x.permute(*torch.arange(x.ndim - 1, -1, -1))` to reverse the dimensions of a tensor. (Triggered internally at /Users/runner/work/_temp/anaconda/conda-bld/pytorch_1711403210267/work/aten/src/ATen/native/TensorShape.cpp:3641.)\n",
      "  plt.imshow(sample.T, origin='lower', cmap=\"viridis\")\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 500x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize a representative training sample\n",
    "batch = next(iter(train_loader))\n",
    "sample = batch[0]  # pick first image from batch (shape: H, W)\n",
    "plt.figure(figsize=(5, 5))\n",
    "plt.imshow(sample.T, origin='lower', cmap=\"viridis\")\n",
    "plt.title(\"Training sample\")\n",
    "plt.axis(\"off\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b8be88",
   "metadata": {},
   "source": [
    "### Defining the VAE\n",
    "\n",
    "Write some text here to explain it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4d33e3a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ReparameterizedDiagonalGaussian(Distribution):\n",
    "    \"\"\"\n",
    "    A distribution `N(y | mu, sigma I)` compatible with the reparameterization trick given `epsilon ~ N(0, 1)`.\n",
    "    \"\"\"\n",
    "    def __init__(self, mu: Tensor, log_sigma:Tensor):\n",
    "        assert mu.shape == log_sigma.shape, f\"Tensors `mu` : {mu.shape} and ` log_sigma` : {log_sigma.shape} must be of the same shape\"\n",
    "        self.mu = mu\n",
    "        self.sigma = log_sigma.exp()\n",
    "        \n",
    "    def sample_epsilon(self) -> Tensor:\n",
    "        \"\"\"`\\eps ~ N(0, I)`\"\"\"\n",
    "        return torch.empty_like(self.mu).normal_()\n",
    "        \n",
    "    def sample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (without gradients)\"\"\"\n",
    "        with torch.no_grad():\n",
    "            return self.rsample()\n",
    "        \n",
    "    def rsample(self) -> Tensor:\n",
    "        \"\"\"sample `z ~ N(z | mu, sigma)` (with the reparameterization trick) \"\"\"\n",
    "        epsilon = self.sample_epsilon()\n",
    "        return self.mu + self.sigma * epsilon\n",
    "            \n",
    "    def log_prob(self, z:Tensor) -> Tensor:\n",
    "        \"\"\"return the log probability: log `p(z)`\"\"\"\n",
    "        return -0.5 * (math.log(2 * math.pi) + 2 * torch.log(self.sigma) + ((z - self.mu) ** 2) / (self.sigma ** 2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9cb5c501",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvolutionalVariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, input_shape:torch.Size, latent_features: int, in_channels: int = 1) -> None:\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.latent_features = latent_features\n",
    "        self.in_channels = in_channels\n",
    "        \n",
    "        # Convolutional encoder keeps the code compact while extracting spatial features.\n",
    "        self.conv_encoder = nn.Sequential(\n",
    "            # Block 1: 128 -> 64\n",
    "            nn.Conv2d(in_channels, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 2: 64 -> 32\n",
    "            nn.Conv2d(32, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 3: 32 -> 16\n",
    "            nn.Conv2d(64, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            # Block 4: 16 -> 8\n",
    "            nn.Conv2d(128, 256, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(256),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            nn.Flatten(),\n",
    "            # Flattened size: 256 channels * 8 * 8 = 16384\n",
    "            nn.Linear(256 * 8 * 8, latent_features * 2) \n",
    "        )\n",
    "\n",
    "        \n",
    "        self.conv_decoder = nn.Sequential(\n",
    "            nn.Linear(latent_features, 256 * 8 * 8), # Upscale linear layer\n",
    "            nn.Unflatten(dim=1, unflattened_size=(256, 8, 8)), # Reshape\n",
    "            \n",
    "            # Block 1: 8 -> 16\n",
    "            nn.ConvTranspose2d(256, 128, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(128),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 2: 16 -> 32\n",
    "            nn.ConvTranspose2d(128, 64, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(64),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 3: 32 -> 64\n",
    "            nn.ConvTranspose2d(64, 32, kernel_size=4, stride=2, padding=1),\n",
    "            nn.BatchNorm2d(32),\n",
    "            nn.ReLU(),\n",
    "            \n",
    "            # Block 4: 64 -> 128\n",
    "            nn.ConvTranspose2d(32, in_channels, kernel_size=4, stride=2, padding=1),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "        \n",
    "        # Prior parameters stay identical to the dense version.\n",
    "        self.register_buffer(\"prior_params\", torch.zeros(1, 2 * latent_features))\n",
    "\n",
    "        # Define the learnable log-scale parameter\n",
    "        self.log_scale = nn.Parameter(torch.tensor([0.0]))\n",
    "\n",
    "        \n",
    "    def posterior(self, x:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\"\"\"\n",
    "\n",
    "        # compute the parameters of the posterior\n",
    "        h_x = self.conv_encoder(x)\n",
    "        mu, log_sigma =  h_x.chunk(2, dim=-1)\n",
    "\n",
    "        log_sigma = torch.clamp(log_sigma,min=-10, max=10)\n",
    "        \n",
    "        # return a distribution `q(x|x) = N(z | \\mu(x), \\sigma(x))`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "        \n",
    "    def prior(self, batch_size:int=1)-> Distribution:\n",
    "        \"\"\"return the distribution `p(z)`\"\"\"\n",
    "        prior_params = self.prior_params.expand(batch_size, *self.prior_params.shape[-1:])\n",
    "        mu, log_sigma = prior_params.chunk(2, dim=-1)\n",
    "        \n",
    "        # return the distribution `p(z)`\n",
    "        return ReparameterizedDiagonalGaussian(mu, log_sigma)\n",
    "    \n",
    "    def observation_model(self, z:Tensor) -> Distribution:\n",
    "        \"\"\"return the distribution `p(x|z)`\"\"\"\n",
    "        mu = self.conv_decoder(z)\n",
    "        #scale = 0.1 # Fixed standard deviation #TODO. look at this\n",
    "        scale = torch.exp(self.log_scale)\n",
    "        return torch.distributions.Normal(mu, scale)\n",
    "        \n",
    "    def forward(self, x) -> Dict[str, Any]:\n",
    "        \"\"\"compute the posterior q(z|x) (encoder), sample z~q(z|x) and return the distribution p(x|z) (decoder)\"\"\"\n",
    "        \n",
    "        # define the posterior q(z|x) / encode x into q(z|x)\n",
    "        qz = self.posterior(x)\n",
    "        \n",
    "        # define the prior p(z)\n",
    "        pz = self.prior(batch_size=x.size(0))\n",
    "        \n",
    "        # sample the posterior using the reparameterization trick: z ~ q(z | x)\n",
    "        z = qz.rsample()\n",
    "        \n",
    "        # define the observation model p(x|z) = B(x | g(z))\n",
    "        px = self.observation_model(z)\n",
    "        \n",
    "        return {'px': px, 'pz': pz, 'qz': qz, 'z': z}\n",
    "        \n",
    "    def sample_from_prior(self, batch_size: int = 16) -> Dict[str, Any]:\n",
    "        pz = self.prior(batch_size=batch_size)\n",
    "        z = pz.rsample()\n",
    "        px = self.observation_model(z)\n",
    "        return {\"px\": px, \"pz\": pz, \"z\": z}\n",
    "\n",
    "# Example usage once you have tensors shaped as (batch, 1, H, W):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0f8f6d46",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reduce(x:Tensor) -> Tensor:\n",
    "    \"\"\"for each datapoint: sum over all dimensions\"\"\"\n",
    "    return x.view(x.size(0), -1).sum(dim=1)\n",
    "\n",
    "class VariationalInference(nn.Module):\n",
    "    def __init__(self, beta:float=1.):\n",
    "        super().__init__()\n",
    "        self.beta = beta\n",
    "        \n",
    "    def forward(self, model:nn.Module, x:Tensor) -> Tuple[Tensor, Dict]:\n",
    "        \n",
    "        # forward pass through the model\n",
    "        outputs = model(x)\n",
    "        \n",
    "        # unpack outputs\n",
    "        px, pz, qz, z = [outputs[k] for k in [\"px\", \"pz\", \"qz\", \"z\"]]\n",
    "        \n",
    "        # evaluate log probabilities\n",
    "        log_px = reduce(px.log_prob(x))\n",
    "        log_pz = reduce(pz.log_prob(z))\n",
    "        log_qz = reduce(qz.log_prob(z))\n",
    "        \n",
    "        # compute the ELBO with and without the beta parameter: \n",
    "        # `L^\\beta = E_q [ log p(x|z) ] - \\beta * D_KL(q(z|x) | p(z))`\n",
    "        # where `D_KL(q(z|x) | p(z)) = log q(z|x) - log p(z)`\n",
    "        kl = log_qz - log_pz\n",
    "        elbo = log_px - kl\n",
    "        beta_elbo = log_px - self.beta * kl\n",
    "        \n",
    "        # loss\n",
    "        loss = -beta_elbo.mean()\n",
    "        \n",
    "        # prepare the output\n",
    "        with torch.no_grad():\n",
    "            diagnostics = {'elbo': elbo, 'log_px':log_px, 'kl': kl}\n",
    "            \n",
    "        return loss, diagnostics, outputs\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "497cdff5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(128, 128)\n",
      "torch.Size([128, 128])\n"
     ]
    }
   ],
   "source": [
    "# Ensure the shape of the data so far\n",
    "print(density_data[22,:,:].shape)\n",
    "dummy = torch.tensor(data[22,:,:], dtype=torch.float32).clone()\n",
    "print(dummy.shape)\n",
    "dummy = dummy.unsqueeze(0).unsqueeze(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fb35383e",
   "metadata": {},
   "source": [
    "### Testing and Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9ac561d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the models, evaluator and optimizer\n",
    "\n",
    "# CVAE\n",
    "latent_features = 32\n",
    "cvae = ConvolutionalVariationalAutoencoder(dummy.shape, latent_features)\n",
    "\n",
    "# Evaluator: Variational Inference\n",
    "beta = 0.75 # Might change\n",
    "vi = VariationalInference(beta=beta)\n",
    "\n",
    "# The Adam optimizer works really well with VAEs.\n",
    "optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-3, weight_decay=1e-6)\n",
    "\n",
    "# define dictionary to store the training curves\n",
    "training_data = defaultdict(list)\n",
    "validation_data = defaultdict(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "72680e35",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">> Using device: cpu\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 30\u001b[0m\n\u001b[1;32m     27\u001b[0m vi\u001b[38;5;241m.\u001b[39mbeta \u001b[38;5;241m=\u001b[39m warmup_factor \u001b[38;5;241m*\u001b[39m target_beta\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# perform a forward pass through the model and compute the ELBO\u001b[39;00m\n\u001b[0;32m---> 30\u001b[0m loss, diagnostics, outputs \u001b[38;5;241m=\u001b[39m \u001b[43mvi\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcvae\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     32\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m     33\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[8], line 13\u001b[0m, in \u001b[0;36mVariationalInference.forward\u001b[0;34m(self, model, x)\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, model:nn\u001b[38;5;241m.\u001b[39mModule, x:Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tuple[Tensor, Dict]:\n\u001b[1;32m     11\u001b[0m     \n\u001b[1;32m     12\u001b[0m     \u001b[38;5;66;03m# forward pass through the model\u001b[39;00m\n\u001b[0;32m---> 13\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;66;03m# unpack outputs\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     px, pz, qz, z \u001b[38;5;241m=\u001b[39m [outputs[k] \u001b[38;5;28;01mfor\u001b[39;00m k \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpx\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mqz\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[7], line 107\u001b[0m, in \u001b[0;36mConvolutionalVariationalAutoencoder.forward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    104\u001b[0m z \u001b[38;5;241m=\u001b[39m qz\u001b[38;5;241m.\u001b[39mrsample()\n\u001b[1;32m    106\u001b[0m \u001b[38;5;66;03m# define the observation model p(x|z) = B(x | g(z))\u001b[39;00m\n\u001b[0;32m--> 107\u001b[0m px \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    109\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpx\u001b[39m\u001b[38;5;124m'\u001b[39m: px, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpz\u001b[39m\u001b[38;5;124m'\u001b[39m: pz, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mqz\u001b[39m\u001b[38;5;124m'\u001b[39m: qz, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mz\u001b[39m\u001b[38;5;124m'\u001b[39m: z}\n",
      "Cell \u001b[0;32mIn[7], line 89\u001b[0m, in \u001b[0;36mConvolutionalVariationalAutoencoder.observation_model\u001b[0;34m(self, z)\u001b[0m\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mobservation_model\u001b[39m(\u001b[38;5;28mself\u001b[39m, z:Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Distribution:\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"return the distribution `p(x|z)`\"\"\"\u001b[39;00m\n\u001b[0;32m---> 89\u001b[0m     mu \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv_decoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mz\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;66;03m#scale = 0.1 # Fixed standard deviation #TODO. look at this\u001b[39;00m\n\u001b[1;32m     91\u001b[0m     scale \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mexp(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog_scale)\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/modules/container.py:217\u001b[0m, in \u001b[0;36mSequential.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[1;32m    216\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[0;32m--> 217\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    218\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/modules/activation.py:101\u001b[0m, in \u001b[0;36mReLU.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minplace\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minplace\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/anaconda3/envs/dl_bout/lib/python3.10/site-packages/torch/nn/functional.py:1473\u001b[0m, in \u001b[0;36mrelu\u001b[0;34m(input, inplace)\u001b[0m\n\u001b[1;32m   1471\u001b[0m     result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrelu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m   1472\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1473\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1474\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "num_epochs = 100\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\">> Using device: {device}\")\n",
    "\n",
    "# move the model to the device\n",
    "cvae = cvae.to(device)\n",
    "\n",
    "\n",
    "# training..\n",
    "epoch = 0\n",
    "while epoch < num_epochs:\n",
    "    epoch+= 1\n",
    "\n",
    "    training_epoch_data = defaultdict(list)\n",
    "    cvae.train()\n",
    "    \n",
    "    # Go through each batch in the training dataset using the loader\n",
    "    # Note that y is not necessarily known as it is here\n",
    "    for x in train_loader:\n",
    "        x = x.to(device)\n",
    "\n",
    "        # KL warm-up parameters\n",
    "        kl_warmup_epochs = 30\n",
    "        target_beta = 0.75  # your existing beta\n",
    "        warmup_factor = min(1.0, epoch / float(kl_warmup_epochs))\n",
    "        vi.beta = warmup_factor * target_beta\n",
    "\n",
    "        # perform a forward pass through the model and compute the ELBO\n",
    "        loss, diagnostics, outputs = vi(cvae, x)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        \n",
    "        # Gradient clip to prevent exploding gradients\n",
    "        torch.nn.utils.clip_grad_norm_(cvae.parameters(), 1.0)\n",
    "\n",
    "        optimizer.step()\n",
    "        \n",
    "        # gather data for the current bach\n",
    "        for k, v in diagnostics.items():\n",
    "            training_epoch_data[k] += [v.mean().item()]\n",
    "      \n",
    "\n",
    "    # gather data for the full epoch\n",
    "    for k, v in training_epoch_data.items():\n",
    "        training_data[k] += [np.mean(training_epoch_data[k])]\n",
    "\n",
    "    # Evaluate on the full test set, do not propagate gradients\n",
    "    with torch.no_grad():\n",
    "        cvae.eval()\n",
    "        \n",
    "        validation_epoch_data = defaultdict(list)\n",
    "        \n",
    "        # Iterate through the entire test loader\n",
    "        for x in test_loader:\n",
    "            x = x.to(device)\n",
    "            \n",
    "            # perform a forward pass through the model and compute the ELBO\n",
    "            loss, diagnostics, outputs = vi(cvae, x)\n",
    "            \n",
    "            # gather data for the validation step\n",
    "            for k, v in diagnostics.items():\n",
    "                validation_epoch_data[k] += [v.mean().item()]\n",
    "        \n",
    "        # Average the metrics over the validation set and store\n",
    "        for k, v in validation_epoch_data.items():\n",
    "            validation_data[k] += [np.mean(v)]\n",
    "    \n",
    "    #print(f\"{epoch} iterations completed\")\n",
    "    # Reproduce the figure from the begining of the notebook, plot the training curves and show latent samples\n",
    "    if epoch % 10 == 0:\n",
    "        #make_plasma_vae_plots(cvae, x, outputs, training_data, validation_data)\n",
    "        clear_output(wait=True)\n",
    "        train_vis = {k: v[2:] for k, v in training_data.items()}\n",
    "        val_vis = {k: v[2:] for k, v in validation_data.items()}\n",
    "        visualize_reconstructions_live(cvae, test_loader, device, epoch, num_samples=3, cmap='viridis', transpose=True)\n",
    "        \n",
    "        plot_training_curves(train_vis, val_vis)\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f66ffaca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved to trained_cvae_model.pt\n"
     ]
    }
   ],
   "source": [
    "# Save trained model\n",
    "model_save_path = Path(\"trained_cvae_model.pt\")\n",
    "torch.save({\n",
    "    'model_state_dict': cvae.state_dict(),\n",
    "    'optimizer_state_dict': optimizer.state_dict(),\n",
    "    'latent_features': latent_features,\n",
    "    'beta': beta,\n",
    "    'epoch': epoch,\n",
    "    'training_data': dict(training_data),\n",
    "    'validation_data': dict(validation_data)\n",
    "}, model_save_path)\n",
    "print(f\"Model saved to {model_save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe8931b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Print training summary after completion\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"TRAINING SUMMARY\")\n",
    "print(\"=\"*70)\n",
    "print(f\"Total epochs trained: {epoch}\")\n",
    "print(f\"Best validation ELBO: {best_val_elbo:.4f}\")\n",
    "print(f\"Final training loss: {training_data['elbo'][-1]:.4f}\")\n",
    "print(f\"Final validation loss: {validation_data['elbo'][-1]:.4f}\")\n",
    "print(f\"Best model saved at: best_cvae.pt\")\n",
    "print(\"=\"*70 + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cf95bac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load trained model\n",
    "model_load_path = Path(\"trained_cvae_model.pt\")\n",
    "\n",
    "if model_load_path.exists():\n",
    "    checkpoint = torch.load(model_load_path)\n",
    "    \n",
    "    # Reconstruct model with same architecture\n",
    "    latent_features = checkpoint['latent_features']\n",
    "    beta = checkpoint['beta']\n",
    "    \n",
    "    cvae = ConvolutionalVariationalAutoencoder(dummy.shape, latent_features)\n",
    "    cvae.load_state_dict(checkpoint['model_state_dict'])\n",
    "    cvae = cvae.to(device)\n",
    "    \n",
    "    # Optionally load optimizer state for continued training\n",
    "    optimizer = torch.optim.Adam(cvae.parameters(), lr=1e-3)\n",
    "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
    "    \n",
    "    # Load training history\n",
    "    training_data = checkpoint['training_data']\n",
    "    validation_data = checkpoint['validation_data']\n",
    "    epoch = checkpoint['epoch']\n",
    "    \n",
    "    print(f\"Model loaded from {model_load_path}\")\n",
    "    print(f\"Trained for {epoch} epochs\")\n",
    "    print(f\"Latent features: {latent_features}, Beta: {beta}\")\n",
    "else:\n",
    "    print(f\"Model file not found at {model_load_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl_bout",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
